# -*- coding: utf-8 -*-
import streamlit as st
import os
from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from langchain.indexes import VectorstoreIndexCreator
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings

# Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© chromadb
try:
    from langchain.vectorstores import Chroma
except ImportError:
    st.warning("Ø¬Ø§Ø±Ù ØªØ«Ø¨ÙŠØª Ø­Ø²Ù… Ø¥Ø¶Ø§ÙÙŠØ©... (Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ø¯Ù‚Ø§Ø¦Ù‚)")
    os.system("pip install chromadb sentence-transformers")
    from langchain.vectorstores import Chroma

# ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
st.set_page_config(page_title="Ù†Ø¸Ø§Ù… Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø©", layout="wide")
st.title("ğŸ§  Ù†Ø¸Ø§Ù… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù† Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø©")

# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª
uploaded_file = st.file_uploader(
    "Ø±ÙØ¹ ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø© (PDF, DOCX, TXT)", 
    type=['pdf', 'docx', 'txt'],
    accept_multiple_files=False
)

# 2. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª
def process_file(uploaded_file):
    try:
        temp_file = f"temp_{uploaded_file.name}"
        with open(temp_file, "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        if uploaded_file.name.endswith('.pdf'):
            loader = PyPDFLoader(temp_file)
        elif uploaded_file.name.endswith('.docx'):
            loader = Docx2txtLoader(temp_file)
        elif uploaded_file.name.endswith('.txt'):
            loader = TextLoader(temp_file, encoding='utf-8')
        else:
            st.error("Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…")
            return None
        
        documents = loader.load()
        os.remove(temp_file)  # Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª
        return documents, loader  # Ø¥Ø±Ø¬Ø§Ø¹ ÙƒÙ„ Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØ§Ù„Ù„ÙˆØ¯Ø±
    
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù: {str(e)}")
        return None, None

# 3. Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª
if uploaded_file:
    documents, loader = process_file(uploaded_file)
    
    if documents and loader:  # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙˆØ§Ù„Ù„ÙˆØ¯Ø±
        st.success(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(documents)} ØµÙØ­Ø© Ø¨Ù†Ø¬Ø§Ø­!")
        
        # Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ Ù„Ù„Ø¨Ø­Ø«
        try:
            index = VectorstoreIndexCreator(
                embedding=HuggingFaceEmbeddings(),
                text_splitter=RecursiveCharacterTextSplitter(
                    chunk_size=1000,
                    chunk_overlap=200
                )
            ).from_loaders([loader])  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù„ÙˆØ¯Ø± Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¹Ø±ÙŠÙÙ‡
            
            retriever = index.vectorstore.as_retriever()
            
            # ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
            st.divider()
            question = st.text_input("Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¹Ù† ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø©:")
            
            if question:
                from langchain.chains import RetrievalQA
                from langchain.llms import OpenAI  # Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯Ù‡
                
                # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ (Ø§Ø³ØªØ¨Ø¯Ù„ Ø¨Ù…ÙØªØ§Ø­ API Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ)
                llm = OpenAI(temperature=0)  # Ø£Ùˆ Ø§Ø³ØªØ®Ø¯Ø§Ù… watsonx ÙƒÙ…Ø§ ÙÙŠ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ØµÙ„ÙŠ
                
                qa = RetrievalQA.from_chain_type(
                    llm=llm,
                    chain_type="stuff",
                    retriever=retriever
                )
                result = qa({"query": question})
                st.subheader("Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:")
                st.write(result["result"])
                
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø©: {str(e)}")

# 4. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©
with st.sidebar:
    st.header("Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
    st.info("""
    **ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:**
    1. Ø§Ø±ÙØ¹ ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„Ù…Ø³Ø§Ø¨Ù‚Ø©
    2. Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ ÙÙŠ Ù…Ø±Ø¨Ø¹ Ø§Ù„Ù†Øµ
    3. Ø§Ø¶ØºØ· Enter Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
    """)
