# -*- coding: utf-8 -*-
import streamlit as st
import os
import tempfile
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.document_loaders import TextLoader
import torch

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
st.set_page_config(page_title="ğŸ§  Ù†Ø¸Ø§Ù… NLP Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…", layout="wide")
st.title("ğŸ“– Ù…Ø­Ù„Ù„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ø°ÙƒÙŠ (Ù…Ø³ØªÙˆÙ‰ Ø§Ø­ØªØ±Ø§ÙÙŠ)")

# ØªØ­Ù…ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ Ù…Ø³Ø¨Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
@st.cache_resource
def load_models():
    # Ù†Ù…ÙˆØ°Ø¬ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
    tokenizer = AutoTokenizer.from_pretrained("UBC-NLP/MARBERT")
    
    # Ù†Ù…ÙˆØ°Ø¬ ÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
    model = AutoModelForSeq2SeqLM.from_pretrained("UBC-NLP/MARBERT")
    
    # Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    nlp_pipeline = pipeline(
        "text2text-generation",
        model=model,
        tokenizer=tokenizer,
        device=0 if torch.cuda.is_available() else -1
    )
    
    # Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
    embeddings = HuggingFaceEmbeddings(
        model_name="uer/sbert-base-arabic-light",
        model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}
    )
    
    return tokenizer, nlp_pipeline, embeddings

tokenizer, nlp_pipeline, embeddings = load_models()

# Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù†ØµÙŠ
def process_text_file(file_path):
    try:
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¹Ø±Ø¨ÙŠ
        loader = TextLoader(file_path, encoding='utf-8')
        documents = loader.load()
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        arabic_splitter = RecursiveCharacterTextSplitter(
            chunk_size=512,
            chunk_overlap=64,
            length_function=lambda x: len(tokenizer.encode(x)),
            separators=["\n\n", "\n", "Û”", "Û” ", " ", ""]
        )
        
        return arabic_splitter.split_documents(documents)
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù: {str(e)}")
        return None

# ÙˆØ§Ø¬Ù‡Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù
uploaded_file = st.file_uploader("Ø±ÙØ¹ Ù…Ù„Ù Ù†ØµÙŠ Ø¹Ø±Ø¨ÙŠ (TXT)", type=['txt'])

if uploaded_file:
    with tempfile.NamedTemporaryFile(delete=False, suffix='.txt') as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_path = tmp_file.name
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù
    documents = process_text_file(tmp_path)
    os.unlink(tmp_path)
    
    if documents:
        st.success(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(documents)} Ù‚Ø³Ù… Ù†ØµÙŠ")
        
        # Ø¥Ù†Ø´Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø¨Ø­Ø« Ø¯Ù„Ø§Ù„ÙŠ
        db = FAISS.from_documents(documents, embeddings)
        st.session_state.db = db
        
        # Ø¹Ø±Ø¶ ØªØ­Ù„ÙŠÙ„ Ø£ÙˆÙ„ÙŠ
        with st.expander("Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù„ØºÙˆÙŠ Ø§Ù„Ø£ÙˆÙ„ÙŠ"):
            sample_text = documents[0].page_content[:500]
            st.write("**Ø§Ù„Ø¹ÙŠÙ†Ø© Ø§Ù„Ù†ØµÙŠØ©:**", sample_text)
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙˆÙƒÙ†Ø²
            tokens = tokenizer.tokenize(sample_text)
            st.write("**Ø¹Ø¯Ø¯ Ø§Ù„ØªÙˆÙƒÙ†Ø²:**", len(tokens))
            st.write("**Ø£ÙˆÙ„ 20 ØªÙˆÙƒÙ†:**", tokens[:20])

# Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
if 'db' in st.session_state:
    st.markdown("---")
    st.subheader("Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")
    
    question = st.text_area("Ø£Ø¯Ø®Ù„ Ø³Ø¤Ø§Ù„Ùƒ Ø§Ù„ØªØ­Ù„ÙŠÙ„ÙŠ (Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©):", height=100)
    
    if question:
        try:
            # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
            docs = st.session_state.db.similarity_search(question, k=3)
            context = "\n\n".join([doc.page_content for doc in docs])
            
            # ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…ØªÙ‚Ø¯Ù…Ø©
            response = nlp_pipeline(
                f"Ø§Ù„Ø³Ø¤Ø§Ù„: {question}\nØ§Ù„Ù†Øµ: {context}",
                max_length=512,
                num_beams=5,
                early_stopping=True
            )
            
            st.markdown("---")
            st.subheader("Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ÙŠØ©:")
            st.write(response[0]['generated_text'])
            
            # Ø¹Ø±Ø¶ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ
            with st.expander("Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠØ©"):
                st.write("**Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:**", context)
                st.write("**Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:**")
                st.json({
                    "model": "MARBERT",
                    "max_length": 512,
                    "num_beams": 5,
                    "early_stopping": True
                })
                
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {str(e)}")

# Ù‚Ø³Ù… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠ
if 'db' in st.session_state:
    st.markdown("---")
    st.subheader("Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†ØµÙŠØ©")
    
    if st.button("ØªØ­Ù„ÙŠÙ„ Ø¥Ø­ØµØ§Ø¦ÙŠ Ù…ØªÙ‚Ø¯Ù…"):
        try:
            all_text = " ".join([doc.page_content for doc in documents])
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙˆÙƒÙ†Ø²
            tokens = tokenizer.tokenize(all_text)
            vocab = set(tokens)
            
            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù†ØµÙŠØ©
            stats = {
                "Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª": len(all_text.split()),
                "Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙˆÙƒÙ†Ø²": len(tokens),
                "Ø­Ø¬Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª": len(vocab),
                "Ø£ÙƒØ«Ø± Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØªÙƒØ±Ø§Ø±Ø§": dict(sorted(
                    {word: tokens.count(word) for word in vocab}.items(),
                    key=lambda item: item[1],
                    reverse=True
                )[:10])
            }
            
            st.write("**Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠ:**")
            st.json(stats)
            
        except Exception as e:
            st.error(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠ: {str(e)}")
