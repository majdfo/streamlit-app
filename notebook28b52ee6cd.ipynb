{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":" \n# Import Langchain dependencies\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.indexes import VectorstoreIndexCreator\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Import Streamlit for UI development\nimport streamlit as st\n\n# Import WatsonX Langchain interface\nfrom watsonxlangchain import LangChainInterface\n\n# Set your WatsonX credentials\ncreds = {\n    'apikey': 'ZNPRw05jup0f2SFOCFkAtfZskeCPL6fESGaYAq1EWRp7',\n    'url': 'https://us-south.ml.cloud.ibm.com'\n}\n\n# Create LLM using Langchain\nllm = LangChainInterface(\n    credentials=creds,\n    model='meta-llama/llama-2-70b-chat',\n    params={\n        'decoding_method': 'sample',\n        'max_new_tokens': 200,\n        'temperature': 0.5\n    },\n    project_id='dd58941e-28cc-4abe-9189-0bebc2f2edec'\n)\n\n# Title of the app\nst.title('Ask WatsonX')\n\n# Setup session state to store messages\nif 'messages' not in st.session_state:\n    st.session_state.messages = []\n\n# Prompt input box\nprompt = st.chat_input('Pass Your Prompt here')\n\n# Display previous messages\nfor message in st.session_state.messages:\n    st.chat_message(message['role']).markdown(message['content'])\n\n# Handle user input\nif prompt:\n    st.chat_message('user').markdown(prompt)\n    st.session_state.messages.append({'role': 'user', 'content': prompt})\n    response = llm(prompt)\n    st.chat_message('assistant').markdown(response)\n    st.session_state.messages.append({'role': 'assistant', 'content': response})\n\n# Cache and load the PDF vector index\n@st.cache_resource\ndef load_pdf():\n    pdf_name = 'what is generative ai.pdf'  # Update if needed\n    loaders = [PyPDFLoader(pdf_name)]\n    index = VectorstoreIndexCreator(\n        embedding=HuggingFaceEmbeddings(model_name='all-MiniLM-L12-v2'),\n        text_splitter=RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n    ).from_loaders(loaders)\n    return index\n\n# Load vector index\nindex = load_pdf()\n\n# Create Q&A retrieval chain from vectorstore\nchain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type='stuff',\n    retriever=index.vectorstore.as_retriever()\n)\n\n# Optional: Enable document-based Q&A\ndoc_question = st.text_input(\"Ask a question about the PDF document\")\nif doc_question:\n    doc_answer = chain.run(doc_question)\n    st.write(\"Answer:\", doc_answer)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}